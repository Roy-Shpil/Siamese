{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 0. Imports",
   "id": "9f358593acccf78f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-04T09:46:07.462089Z",
     "start_time": "2025-06-04T09:45:52.185338Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from utils.general_utils import load_data, load_files, study_best_params\n",
    "from utils.nn_utils import train_model_nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roysh\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\code\\prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roysh\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\venv\\Scripts\\python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Loads",
   "id": "89c8d59c7857fdd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1. Loading train and test filepaths and labels",
   "id": "fd122387415ccec6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:46:36.713378Z",
     "start_time": "2025-06-04T09:46:07.484234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_data = load_files('../../data/processed_data')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_val_set_1, train_val_set_2 = load_data(processed_data,  '../../data/full_dataset', 'train')\n",
    "test_set_1, test_set_2 = load_data(processed_data, '../../data/full_dataset', 'test')\n",
    "train_val_labels = torch.Tensor(processed_data['train_labels']).reshape(-1, 1)\n",
    "test_labels = torch.Tensor(processed_data['test_labels']).reshape(-1, 1)\n"
   ],
   "id": "bd1f9ee8cdfa5205",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T10:39:46.231814Z",
     "start_time": "2025-06-04T10:39:19.864192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import pickle\n",
    "names_to_save = ['train_val_set_1', 'train_val_set_2', 'test_set_1', 'test_set_2', 'train_val_labels', 'test_labels']\n",
    "vars_to_save = [train_val_set_1, train_val_set_2, test_set_1, test_set_2, train_val_labels, test_labels]\n",
    "for i, name in enumerate(names_to_save):\n",
    "    with open(f'../../data/to_colab/{name}', 'wb') as f:\n",
    "        pickle.dump(vars_to_save[i], f)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "b1bf7b8e670eb12",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:49:31.446865Z",
     "start_time": "2025-06-04T09:49:31.440408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    'batch_size': {'suggestion': 'categorical', 'choices': [32, 64, 128], 'listed': None},\n",
    "    'activation': {'suggestion': 'categorical', 'choices': [\"ReLU\", \"leaky_relu\", \"GELU\", \"ELU\"], 'listed': None},\n",
    "    'scheduler': {'suggestion': 'categorical', 'choices': [\"linear\", \"cosine\", \"plateau\"], 'listed': None},\n",
    "    'num_epochs': {'suggestion': 'int', 'low': 1, 'high': 1, 'log': False, 'listed': None},\n",
    "    'learning_rate': {'suggestion': 'float', 'low': 1e-6, 'high': 1e-2, 'log': True, 'listed': None},\n",
    "    'num_convs': {'suggestion': 'int', 'low': 1, 'high': 5, 'log': False, 'listed': None},\n",
    "    'channels': {'suggestion': 'int', 'low': 4, 'high': 512, 'log': True, 'listed': 'num_convs'},\n",
    "    'kernel_size': {'suggestion': 'categorical', 'choices': [3, 5, 7], 'listed': 'num_convs'}\n",
    "}\n",
    "\n",
    "params_initial = {\n",
    "    'batch_size': 128,\n",
    "    'activation': 'ReLU',\n",
    "    'scheduler': \"cosine\",\n",
    "    'num_epochs': 20,\n",
    "    'learning_rate': 3e-6,\n",
    "    'num_convs': 5,\n",
    "    'channels': [32, 64, 128, 128, 256],\n",
    "    'kernel_size': [11, 6, 4, 4, 3]\n",
    "}\n",
    "\n",
    "params_trash = {\n",
    "    'batch_size': 16,\n",
    "    'activation': 'ReLU',\n",
    "    'scheduler': \"cosine\",\n",
    "    'num_epochs': 1,\n",
    "    'learning_rate': 3e-6,\n",
    "    'num_convs': 5,\n",
    "    'channels': [1, 1, 1, 1, 1],\n",
    "    'kernel_size': [5, 5, 5, 5, 5]\n",
    "}"
   ],
   "id": "47fb7ef0fb95e521",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:49:32.769256Z",
     "start_time": "2025-06-04T09:49:32.712469Z"
    }
   },
   "cell_type": "code",
   "source": "train_1, val_1, train_2, val_2, train_out, val_out = train_test_split(train_val_set_1_cut, train_val_set_2_cut, train_val_labels_cut, test_size=0.2, random_state=42)\n",
   "id": "fcf181aa7aa4df3d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:49:33.421646Z",
     "start_time": "2025-06-04T09:49:33.408823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "val_loss, model, scaler = train_model_nn(train_1, train_2, train_out, val_1, val_2, val_out, params_trash, check_val=True)\n",
    "print(\"wtg\")\n",
    "\"\"\"\n"
   ],
   "id": "bed8d33e8dc958ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nval_loss, model, scaler = train_model_nn(train_1, train_2, train_out, val_1, val_2, val_out, params_trash, check_val=True)\\nprint(\"wtg\")\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T09:51:36.394600Z",
     "start_time": "2025-06-04T09:49:33.989256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models, scalers, scores = study_best_params(train_val_set_1,\n",
    "                                            train_val_set_2,\n",
    "                                            train_val_labels,\n",
    "                                            search_space,\n",
    "                                            device,\n",
    "                                            k=5,\n",
    "                                            print_message=False,\n",
    "                                            num_iters=30)"
   ],
   "id": "cce9633c75304a20",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 12:49:33,999] A new study created in memory with name: no-name-6f86ce34-397f-4b43-89af-453d387818c8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters: {'batch_size': 32, 'activation': 'ReLU', 'scheduler': 'linear', 'num_epochs': 1, 'learning_rate': 0.0015500066798583873, 'num_convs': 4, 'channels': [9, 472, 36, 291], 'kernel_size': [7, 3, 7, 7]}\n",
      "\n",
      "\n",
      "training fold no. 0\n",
      "batch: 1 out of 1. loss: 0.5504733324050903\n",
      "epoch: 0, loss: 0.5504733324050903, val_loss: 0.7829428911209106\n",
      "training fold no. 1\n",
      "batch: 1 out of 1. loss: 0.5733436346054077\n",
      "epoch: 0, loss: 0.5733436346054077, val_loss: 0.6623272895812988\n",
      "training fold no. 2\n",
      "batch: 1 out of 1. loss: 0.5466167330741882\n",
      "epoch: 0, loss: 0.5466167330741882, val_loss: 0.6293247938156128\n",
      "training fold no. 3\n",
      "batch: 1 out of 1. loss: 0.7666460871696472\n",
      "epoch: 0, loss: 0.7666460871696472, val_loss: 0.6936387419700623\n",
      "training fold no. 4\n",
      "batch: 1 out of 1. loss: 0.6842445731163025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 12:50:02,763] Trial 0 finished with value: 0.6984061002731323 and parameters: {'batch_size': 32, 'activation': 'ReLU', 'scheduler': 'linear', 'num_epochs': 1, 'learning_rate': 0.0015500066798583873, 'num_convs': 4, 'channels_0': 9, 'channels_1': 472, 'channels_2': 36, 'channels_3': 291, 'kernel_size_0': 7, 'kernel_size_1': 3, 'kernel_size_2': 7, 'kernel_size_3': 7}. Best is trial 0 with value: 0.6984061002731323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.6842445731163025, val_loss: 0.7237967252731323\n",
      "KFold complete! Final Validation Loss: 0.6984061002731323\n",
      "\n",
      "Parameters: {'batch_size': 64, 'activation': 'ELU', 'scheduler': 'plateau', 'num_epochs': 1, 'learning_rate': 0.004306648371387978, 'num_convs': 5, 'channels': [17, 86, 17, 43, 13], 'kernel_size': [3, 3, 3, 5, 5]}\n",
      "\n",
      "\n",
      "training fold no. 0\n",
      "batch: 1 out of 1. loss: 0.5930120944976807\n",
      "epoch: 0, loss: 0.5930120944976807, val_loss: 0.6830834150314331\n",
      "training fold no. 1\n",
      "batch: 1 out of 1. loss: 0.7850885987281799\n",
      "epoch: 0, loss: 0.7850885987281799, val_loss: 0.7051235437393188\n",
      "training fold no. 2\n",
      "batch: 1 out of 1. loss: 0.8380025625228882\n",
      "epoch: 0, loss: 0.8380025625228882, val_loss: 0.6229910850524902\n",
      "training fold no. 3\n",
      "batch: 1 out of 1. loss: 0.6962013244628906\n",
      "epoch: 0, loss: 0.6962013244628906, val_loss: 0.7581342458724976\n",
      "training fold no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 12:50:09,166] Trial 1 finished with value: 0.699020504951477 and parameters: {'batch_size': 64, 'activation': 'ELU', 'scheduler': 'plateau', 'num_epochs': 1, 'learning_rate': 0.004306648371387978, 'num_convs': 5, 'channels_0': 17, 'channels_1': 86, 'channels_2': 17, 'channels_3': 43, 'channels_4': 13, 'kernel_size_0': 3, 'kernel_size_1': 3, 'kernel_size_2': 3, 'kernel_size_3': 5, 'kernel_size_4': 5}. Best is trial 0 with value: 0.6984061002731323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1 out of 1. loss: 0.6526319980621338\n",
      "epoch: 0, loss: 0.6526319980621338, val_loss: 0.7257701754570007\n",
      "KFold complete! Final Validation Loss: 0.699020504951477\n",
      "\n",
      "Parameters: {'batch_size': 32, 'activation': 'ReLU', 'scheduler': 'cosine', 'num_epochs': 1, 'learning_rate': 0.0006951381883743046, 'num_convs': 5, 'channels': [4, 25, 24, 5, 4], 'kernel_size': [5, 5, 3, 7, 3]}\n",
      "\n",
      "\n",
      "training fold no. 0\n",
      "batch: 1 out of 1. loss: 0.8295272588729858\n",
      "epoch: 0, loss: 0.8295272588729858, val_loss: 0.6964078545570374\n",
      "training fold no. 1\n",
      "batch: 1 out of 1. loss: 0.7225533723831177\n",
      "epoch: 0, loss: 0.7225533723831177, val_loss: 0.6779953241348267\n",
      "training fold no. 2\n",
      "batch: 1 out of 1. loss: 0.7904398441314697\n",
      "epoch: 0, loss: 0.7904398441314697, val_loss: 0.6609019041061401\n",
      "training fold no. 3\n",
      "batch: 1 out of 1. loss: 0.6833068132400513\n",
      "epoch: 0, loss: 0.6833068132400513, val_loss: 0.7168624401092529\n",
      "training fold no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 12:50:11,727] Trial 2 finished with value: 0.6970920562744141 and parameters: {'batch_size': 32, 'activation': 'ReLU', 'scheduler': 'cosine', 'num_epochs': 1, 'learning_rate': 0.0006951381883743046, 'num_convs': 5, 'channels_0': 4, 'channels_1': 25, 'channels_2': 24, 'channels_3': 5, 'channels_4': 4, 'kernel_size_0': 5, 'kernel_size_1': 5, 'kernel_size_2': 3, 'kernel_size_3': 7, 'kernel_size_4': 3}. Best is trial 2 with value: 0.6970920562744141.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1 out of 1. loss: 0.857418954372406\n",
      "epoch: 0, loss: 0.857418954372406, val_loss: 0.733292818069458\n",
      "KFold complete! Final Validation Loss: 0.6970920562744141\n",
      "\n",
      "Parameters: {'batch_size': 64, 'activation': 'leaky_relu', 'scheduler': 'linear', 'num_epochs': 1, 'learning_rate': 4.51488254504099e-06, 'num_convs': 2, 'channels': [44, 7], 'kernel_size': [5, 7]}\n",
      "\n",
      "\n",
      "training fold no. 0\n",
      "batch: 1 out of 1. loss: 0.5239120125770569\n",
      "epoch: 0, loss: 0.5239120125770569, val_loss: 0.6865122318267822\n",
      "training fold no. 1\n",
      "batch: 1 out of 1. loss: 0.7162191271781921\n",
      "epoch: 0, loss: 0.7162191271781921, val_loss: 0.7019116878509521\n",
      "training fold no. 2\n",
      "batch: 1 out of 1. loss: 0.5940818190574646\n",
      "epoch: 0, loss: 0.5940818190574646, val_loss: 0.7339673042297363\n",
      "training fold no. 3\n",
      "batch: 1 out of 1. loss: 0.6195225715637207\n",
      "epoch: 0, loss: 0.6195225715637207, val_loss: 0.7037011981010437\n",
      "training fold no. 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-04 12:50:19,222] Trial 3 finished with value: 0.705429196357727 and parameters: {'batch_size': 64, 'activation': 'leaky_relu', 'scheduler': 'linear', 'num_epochs': 1, 'learning_rate': 4.51488254504099e-06, 'num_convs': 2, 'channels_0': 44, 'channels_1': 7, 'kernel_size_0': 5, 'kernel_size_1': 7}. Best is trial 2 with value: 0.6970920562744141.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 1 out of 1. loss: 0.7126471996307373\n",
      "epoch: 0, loss: 0.7126471996307373, val_loss: 0.701053261756897\n",
      "KFold complete! Final Validation Loss: 0.705429196357727\n",
      "\n",
      "Parameters: {'batch_size': 32, 'activation': 'leaky_relu', 'scheduler': 'cosine', 'num_epochs': 1, 'learning_rate': 1.0909622007071157e-06, 'num_convs': 2, 'channels': [395, 25], 'kernel_size': [3, 3]}\n",
      "\n",
      "\n",
      "training fold no. 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-06-04 12:51:35,663] Trial 4 failed with parameters: {'batch_size': 32, 'activation': 'leaky_relu', 'scheduler': 'cosine', 'num_epochs': 1, 'learning_rate': 1.0909622007071157e-06, 'num_convs': 2, 'channels_0': 395, 'channels_1': 25, 'kernel_size_0': 3, 'kernel_size_1': 3} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\roysh\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\venv\\Scripts\\python\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\roysh\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\code\\prediction\\utils\\general_utils.py\", line 42, in objective\n",
      "    val_loss, models_kfold, scalers_kfold = k_fold_cross_val(X_train_val_1, X_train_val_2, y_train_val, params, k,\n",
      "                                            ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                 print_message)\n",
      "                 ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\roysh\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\code\\prediction\\utils\\general_utils.py\", line 103, in k_fold_cross_val\n",
      "    val_loss, model, scaler = train_model_nn(X_train_1, X_train_2, y_train, X_val_1, X_val_2, y_val, params, check_val=True)\n",
      "                              ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\roysh\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\code\\prediction\\utils\\nn_utils.py\", line 105, in train_model_nn\n",
      "    loss.backward()\n",
      "    ~~~~~~~~~~~~~^^\n",
      "  File \"C:\\Users\\roysh\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\venv\\Scripts\\python\\Lib\\site-packages\\torch\\_tensor.py\", line 648, in backward\n",
      "    torch.autograd.backward(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self, gradient, retain_graph, create_graph, inputs=inputs\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\roysh\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\venv\\Scripts\\python\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "    ~~~~~~~~~~~~~~~~~~~~^\n",
      "        tensors,\n",
      "        ^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        accumulate_grad=True,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\roysh\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\venv\\Scripts\\python\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        t_outputs, *args, **kwargs\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )  # Calls into the C++ engine to run the backward pass\n",
      "    ^\n",
      "KeyboardInterrupt\n",
      "[W 2025-06-04 12:51:35,756] Trial 4 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m models, scalers, scores = \u001B[43mstudy_best_params\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_val_set_1_cut\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_val_set_2_cut\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_val_labels_cut\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msearch_space\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_iters\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\code\\prediction\\utils\\general_utils.py:118\u001B[39m, in \u001B[36mstudy_best_params\u001B[39m\u001B[34m(X_train_val_1, X_train_val_2, y_train_val, param_space, k, print_message, num_iters)\u001B[39m\n\u001B[32m    116\u001B[39m study = optuna.create_study(direction=\u001B[33m\"\u001B[39m\u001B[33mminimize\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    117\u001B[39m objective = make_objective(X_train_val_1, X_train_val_2, y_train_val, param_space, k, print_message)\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m \u001B[43mstudy\u001B[49m\u001B[43m.\u001B[49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobjective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_iters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43msave_best_model\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    120\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mBest hyperparameters:\u001B[39m\u001B[33m\"\u001B[39m, study.best_params)\n\u001B[32m    122\u001B[39m models = study.user_attrs[\u001B[33m\"\u001B[39m\u001B[33mbest_model\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\venv\\Scripts\\python\\Lib\\site-packages\\optuna\\study\\study.py:475\u001B[39m, in \u001B[36mStudy.optimize\u001B[39m\u001B[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[39m\n\u001B[32m    373\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34moptimize\u001B[39m(\n\u001B[32m    374\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    375\u001B[39m     func: ObjectiveFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m    382\u001B[39m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    383\u001B[39m ) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    384\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[32m    385\u001B[39m \n\u001B[32m    386\u001B[39m \u001B[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    473\u001B[39m \u001B[33;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[32m    474\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m475\u001B[39m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    476\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    477\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    478\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    479\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    480\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    481\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    482\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    483\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    484\u001B[39m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    485\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\venv\\Scripts\\python\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001B[39m, in \u001B[36m_optimize\u001B[39m\u001B[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[39m\n\u001B[32m     61\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     62\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs == \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m63\u001B[39m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     64\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     66\u001B[39m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     70\u001B[39m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     71\u001B[39m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     72\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     73\u001B[39m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     76\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs == -\u001B[32m1\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\venv\\Scripts\\python\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001B[39m, in \u001B[36m_optimize_sequential\u001B[39m\u001B[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[39m\n\u001B[32m    157\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m    159\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m160\u001B[39m     frozen_trial = \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    162\u001B[39m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[32m    163\u001B[39m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[32m    164\u001B[39m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[32m    165\u001B[39m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[32m    166\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\venv\\Scripts\\python\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001B[39m, in \u001B[36m_run_trial\u001B[39m\u001B[34m(study, func, catch)\u001B[39m\n\u001B[32m    241\u001B[39m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mShould not reach.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    243\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    244\u001B[39m     frozen_trial.state == TrialState.FAIL\n\u001B[32m    245\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    246\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[32m    247\u001B[39m ):\n\u001B[32m--> \u001B[39m\u001B[32m248\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\venv\\Scripts\\python\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001B[39m, in \u001B[36m_run_trial\u001B[39m\u001B[34m(study, func, catch)\u001B[39m\n\u001B[32m    195\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001B[32m    196\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m197\u001B[39m         value_or_values = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    198\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions.TrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    199\u001B[39m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[32m    200\u001B[39m         state = TrialState.PRUNED\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\code\\prediction\\utils\\general_utils.py:42\u001B[39m, in \u001B[36mmake_objective.<locals>.objective\u001B[39m\u001B[34m(trial)\u001B[39m\n\u001B[32m     40\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mobjective\u001B[39m(trial):\n\u001B[32m     41\u001B[39m     params = suggest_params(trial, param_space)\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m     val_loss, models_kfold, scalers_kfold = \u001B[43mk_fold_cross_val\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_val_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train_val_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     43\u001B[39m \u001B[43m                 \u001B[49m\u001B[43mprint_message\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     44\u001B[39m     trial.set_user_attr(\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, models_kfold)\n\u001B[32m     45\u001B[39m     trial.set_user_attr(\u001B[33m\"\u001B[39m\u001B[33mscaler\u001B[39m\u001B[33m\"\u001B[39m, scalers_kfold)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\code\\prediction\\utils\\general_utils.py:103\u001B[39m, in \u001B[36mk_fold_cross_val\u001B[39m\u001B[34m(X_train_val_1, X_train_val_2, y_train_val, params, k, print_message)\u001B[39m\n\u001B[32m    101\u001B[39m X_train_2, X_val_2 = X_train_val_2[train_idx, :, :, :], X_train_val_2[val_idx, :, :, :]\n\u001B[32m    102\u001B[39m y_train, y_val = y_train_val[train_idx, :], y_train_val[val_idx, :]\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m val_loss, model, scaler = \u001B[43mtrain_model_nn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcheck_val\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    104\u001B[39m k_val_losses.append(val_loss)\n\u001B[32m    105\u001B[39m k_fold_models.append(model)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\code\\prediction\\utils\\nn_utils.py:105\u001B[39m, in \u001B[36mtrain_model_nn\u001B[39m\u001B[34m(X_train_1, X_train_2, y_train, X_val_1, X_val_2, y_val, params, check_val)\u001B[39m\n\u001B[32m    103\u001B[39m y_pred = model(X_1_batch, X_2_batch)\n\u001B[32m    104\u001B[39m loss = criterion(y_pred, y_batch)\n\u001B[32m--> \u001B[39m\u001B[32m105\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    106\u001B[39m optimizer.step()\n\u001B[32m    107\u001B[39m epoch_loss += loss.item() * X_1_batch.size(\u001B[32m0\u001B[39m)/\u001B[38;5;28mlen\u001B[39m(train_loader.dataset)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\venv\\Scripts\\python\\Lib\\site-packages\\torch\\_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\venv\\Scripts\\python\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Roy\\Army\\Hacshara\\Siamese\\Siamese\\venv\\Scripts\\python\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "60edaa85ed94c109",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
